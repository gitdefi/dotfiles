#
# /etc/sysctl.conf - Configuration file for setting system variables
# See /etc/sysctl.d/ for additional system variables.
# See sysctl.conf (5) for information.
#

#kernel.domainname = example.com

# Uncomment the following to stop low-level messages on console
#kernel.printk = 3 4 1 3

###################################################################
# Functions previously found in netbase
#

# Uncomment the next two lines to enable Spoof protection (reverse-path filter)
# Turn on Source Address Verification in all interfaces to
# prevent some spoofing attacks
#net.ipv4.conf.default.rp_filter=1
#net.ipv4.conf.all.rp_filter=1

# Uncomment the next line to enable TCP/IP SYN cookies
# See http://lwn.net/Articles/277146/
# Note: This may impact IPv6 TCP sessions too
#net.ipv4.tcp_syncookies=1

# Uncomment the next line to enable packet forwarding for IPv4
#net.ipv4.ip_forward=1

# Uncomment the next line to enable packet forwarding for IPv6
#  Enabling this option disables Stateless Address Autoconfiguration
#  based on Router Advertisements for this host
#net.ipv6.conf.all.forwarding=1


###################################################################
# Additional settings - these settings can improve the network
# security of the host and prevent against some network attacks
# including spoofing attacks and man in the middle attacks through
# redirection. Some network environments, however, require that these
# settings are disabled so review and enable them as needed.
#
# Do not accept ICMP redirects (prevent MITM attacks)
#net.ipv4.conf.all.accept_redirects = 0
#net.ipv6.conf.all.accept_redirects = 0
# _or_
# Accept ICMP redirects only for gateways listed in our default
# gateway list (enabled by default)
# net.ipv4.conf.all.secure_redirects = 1
#
# Do not send ICMP redirects (we are not a router)
#net.ipv4.conf.all.send_redirects = 0
#
# Do not accept IP source route packets (we are not a router)
#net.ipv4.conf.all.accept_source_route = 0
#net.ipv6.conf.all.accept_source_route = 0
#
# Log Martian Packets
#net.ipv4.conf.all.log_martians = 1
#

###################################################################
# Magic system request Key
# 0=disable, 1=enable all, >1 bitmask of sysrq functions
# See https://www.kernel.org/doc/html/latest/admin-guide/sysrq.html
# for what other values do
#kernel.sysrq=438

net.ipv4.tcp_no_metrics_save=1
net.ipv4.tcp_ecn=0
net.ipv4.tcp_frto=0
net.ipv4.tcp_mtu_probing=0
net.ipv4.tcp_rfc1337=0
net.ipv4.tcp_sack=1
net.ipv4.tcp_fack=1
net.ipv4.tcp_window_scaling=1
net.ipv4.tcp_adv_win_scale=1
net.ipv4.tcp_moderate_rcvbuf=1
# net.core.rmem_max=33554432
# net.core.wmem_max=33554432
net.ipv4.tcp_rmem=4096 87380 33554432
net.ipv4.tcp_wmem=4096 16384 33554432
net.ipv4.udp_rmem_min=8192
net.ipv4.udp_wmem_min=8192
net.core.default_qdisc=fq
net.ipv4.tcp_congestion_control=bbr
net.ipv4.conf.all.route_localnet=1
net.ipv4.ip_forward=1
net.ipv4.conf.all.forwarding=1
net.ipv4.conf.default.forwarding=1
fs.file-max=1000000
# # >
net.ipv6.conf.all.forwarding=1
net.ipv6.conf.default.forwarding=1
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.rp_filter=0
# # BP net.ipv4.conf.all.arp_filter=0
# # BP net.ipv4.conf.default.arp_filter=0
# > 自定义最大接收和发送缓冲区大小
# Increase buffer sizes for better network performance
net.core.rmem_max=600000000
net.core.wmem_max=600000000
# > BWG
net.ipv4.neigh.default.base_reachable_time_ms = 600000
net.ipv4.neigh.eth0.delay_first_probe_time = 1
net.ipv4.neigh.eth1.delay_first_probe_time = 1
net.ipv4.neigh.default.mcast_solicit = 20
net.ipv4.neigh.default.retrans_time_ms = 250
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.eth0.rp_filter=0
net.ipv4.conf.eth1.rp_filter=0
# net.ipv4.conf.eth1.rp_filter=0
net.core.default_qdisc=fq
net.ipv4.tcp_congestion_control=bbr
net.ipv4.tcp_fastopen=3


# # >


# ------ 网络调优: 基本 ------
# TTL 配置, Linux 默认 64
# net.ipv4.ip_default_ttl=64

# 参阅 RFC 1323. 应当启用.
net.ipv4.tcp_timestamps=1
# ------ END 网络调优: 基本 ------

# ------ 网络调优: 内核 Backlog 队列和缓存相关 ------
# Ref: https://www.starduster.me/2020/03/02/linux-network-tuning-kernel-parameter/
# Ref: https://blog.cloudflare.com/optimizing-tcp-for-high-throughput-and-low-latency/
# Ref: https://zhuanlan.zhihu.com/p/149372947
# 有条件建议依据实测结果调整相关数值
# 缓冲区相关配置均和内存相关
net.core.wmem_default=16384
net.core.rmem_default=262144
net.core.rmem_max=536870912
net.core.wmem_max=536870912
net.ipv4.tcp_rmem=8192 262144 536870912
net.ipv4.tcp_wmem=4096 16384 536870912
net.ipv4.tcp_adv_win_scale=-2
net.ipv4.tcp_collapse_max_bytes=6291456
net.ipv4.tcp_notsent_lowat=131072
net.core.netdev_max_backlog=10240
net.ipv4.tcp_max_syn_backlog=10240
net.core.somaxconn=8192
net.ipv4.tcp_abort_on_overflow=1
# 流控和拥塞控制相关调优
# Egress traffic control 相关. 可选 fq, cake
# 实测二者区别不大, 保持默认 fq 即可
net.core.default_qdisc=fq_pie
# Xanmod 内核 6.X 版本目前默认使用 bbr3, 无需设置
# 实测比 bbr, bbr2 均有提升
# 不过网络条件不同会影响. 有需求请实测.
# net.ipv4.tcp_congestion_control=bbr3
# 显式拥塞通知
# 已被发现在高度拥塞的网络上是有害的.
# net.ipv4.tcp_ecn=1
# TCP 自动窗口
# 要支持超过 64KB 的 TCP 窗口必须启用
net.ipv4.tcp_window_scaling=1
# 开启后, TCP 拥塞窗口会在一个 RTO 时间
# 空闲之后重置为初始拥塞窗口 (CWND) 大小.
# 大部分情况下, 尤其是大流量长连接, 设置为 0.
# 对于网络情况时刻在相对剧烈变化的场景, 设置为 1.
net.ipv4.tcp_slow_start_after_idle=0
# nf_conntrack 调优
# Add Ref: https://gist.github.com/lixingcong/0e13b4123d29a465e364e230b2e45f60
net.nf_conntrack_max=1000000
net.netfilter.nf_conntrack_max=1000000
net.netfilter.nf_conntrack_tcp_timeout_fin_wait=30
net.netfilter.nf_conntrack_tcp_timeout_time_wait=30
net.netfilter.nf_conntrack_tcp_timeout_close_wait=15
net.netfilter.nf_conntrack_tcp_timeout_established=300
net.ipv4.netfilter.ip_conntrack_tcp_timeout_established=7200
# TIME-WAIT 状态调优
# Ref: http://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux.html
# Ref: https://www.cnblogs.com/lulu/p/4149312.html
# 4.12 内核中此参数已经永久废弃, 不用纠结是否需要开启
# net.ipv4.tcp_tw_recycle=0
## 只对客户端生效, 服务器连接上游时也认为是客户端
net.ipv4.tcp_tw_reuse=1
# 系统同时保持TIME_WAIT套接字的最大数量
# 如果超过这个数字 TIME_WAIT 套接字将立刻被清除
net.ipv4.tcp_max_tw_buckets=55000
# ------ END 网络调优: 内核 Backlog 队列和缓存相关 ------

# ------ 网络调优: 其他 ------
# Ref: https://zhuanlan.zhihu.com/p/149372947
# Ref: https://www.starduster.me/2020/03/02/linux-network-tuning-kernel-parameter/#netipv4tcp_max_syn_backlog_netipv4tcp_syncookies
# 启用选择应答
# 对于广域网通信应当启用
net.ipv4.tcp_sack=1
# 启用转发应答
# 对于广域网通信应当启用
net.ipv4.tcp_fack=1
# TCP SYN 连接超时重传次数
net.ipv4.tcp_syn_retries=3
net.ipv4.tcp_synack_retries=3
# TCP SYN 连接超时时间, 设置为 5 约为 30s
net.ipv4.tcp_retries2=5
# 开启 SYN 洪水攻击保护
# 注意: tcp_syncookies 启用时, 此时实际上没有逻辑上的队列长度,
# Backlog 设置将被忽略. syncookie 是一个出于对现实的妥协,
# 严重违反 TCP 协议的设计, 会造成 TCP option 不可用, 且实现上
# 通过计算 hash 避免维护半开连接也是一种 tradeoff 而非万金油,
# 勿听信所谓“安全优化教程”而无脑开启
net.ipv4.tcp_syncookies=0

# Ref: https://linuxgeeks.github.io/2017/03/20/212135-Linux%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0rp_filter/
# 开启反向路径过滤
# Aliyun 负载均衡实例后端的 ECS 需要设置为 0
net.ipv4.conf.default.rp_filter=2
net.ipv4.conf.all.rp_filter=2

# 减少处于 FIN-WAIT-2 连接状态的时间使系统可以处理更多的连接
# Ref: https://www.cnblogs.com/kaishirenshi/p/11544874.html
net.ipv4.tcp_fin_timeout=10

# Ref: https://xwl-note.readthedocs.io/en/latest/linux/tuning.html
# 默认情况下一个 TCP 连接关闭后, 把这个连接曾经有的参数保存到dst_entry中
# 只要 dst_entry 没有失效,下次新建立相同连接的时候就可以使用保存的参数来初始化这个连接.通常情况下是关闭的
net.ipv4.tcp_no_metrics_save=1
# unix socket 最大队列
net.unix.max_dgram_qlen=1024
# 路由缓存刷新频率
net.ipv4.route.gc_timeout=100

# Ref: https://gist.github.com/lixingcong/0e13b4123d29a465e364e230b2e45f60
# 启用 MTU 探测，在链路上存在 ICMP 黑洞时候有用（大多数情况是这样）
net.ipv4.tcp_mtu_probing = 1

# No Ref
# 开启并记录欺骗, 源路由和重定向包
net.ipv4.conf.all.log_martians=1
net.ipv4.conf.default.log_martians=1
# 处理无源路由的包
net.ipv4.conf.all.accept_source_route=0
net.ipv4.conf.default.accept_source_route=0
# TCP KeepAlive 调优
# 最大闲置时间
net.ipv4.tcp_keepalive_time=300
# 最大失败次数, 超过此值后将通知应用层连接失效
net.ipv4.tcp_keepalive_probes=2
# 发送探测包的时间间隔
net.ipv4.tcp_keepalive_intvl=2
# 系统所能处理不属于任何进程的TCP sockets最大数量
net.ipv4.tcp_max_orphans=262144
# arp_table的缓存限制优化
net.ipv4.neigh.default.gc_thresh1=128
net.ipv4.neigh.default.gc_thresh2=512
net.ipv4.neigh.default.gc_thresh3=4096
net.ipv4.neigh.default.gc_stale_time=120
net.ipv4.conf.default.arp_announce=2
net.ipv4.conf.lo.arp_announce=2
net.ipv4.conf.all.arp_announce=2
# ------ END 网络调优: 其他 ------

# ------ 内核调优 ------

# Ref: Aliyun, etc
# 内核 Panic 后 1 秒自动重启
kernel.panic=1
# 允许更多的PIDs, 减少滚动翻转问题
kernel.pid_max=32768
# 内核所允许的最大共享内存段的大小（bytes）
kernel.shmmax=4294967296
# 在任何给定时刻, 系统上可以使用的共享内存的总量（pages）
kernel.shmall=1073741824
# 设定程序core时生成的文件名格式
kernel.core_pattern=core_%e
# 当发生oom时, 自动转换为panic
vm.panic_on_oom=1
# 表示强制Linux VM最低保留多少空闲内存（Kbytes）
# vm.min_free_kbytes=1048576
# 该值高于100, 则将导致内核倾向于回收directory和inode cache
vm.vfs_cache_pressure=250
# 表示系统进行交换行为的程度, 数值（0-100）越高, 越可能发生磁盘交换
vm.swappiness=10
# 仅用10%做为系统cache
vm.dirty_ratio=10
vm.overcommit_memory=1
# 增加系统文件描述符限制
# Fix error: too many open files
fs.file-max=1048575
fs.inotify.max_user_instances=8192
fs.inotify.max_user_instances=8192
# 内核响应魔术键
kernel.sysrq=1
# 弃用
# net.ipv4.tcp_low_latency=1

# Ref: https://gist.github.com/lixingcong/0e13b4123d29a465e364e230b2e45f60
# 当某个节点可用内存不足时, 系统会倾向于从其他节点分配内存. 对 Mongo/Redis 类 cache 服务器友好
vm.zone_reclaim_mode=0


# # # >


net.core.default_qdisc = fq_codel
net.ipv4.tcp_congestion_control = bbr
net.core.somaxconn = 8192
net.core.rmem_max = 67108848
net.core.wmem_max = 67108848
net.core.netdev_max_backlog = 16384
net.ipv4.tcp_sack = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_timestamps = 1
net.ipv4.tcp_adv_win_scale = -2
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_max_tw_buckets = 6000
net.ipv4.tcp_max_syn_backlog = 8192
net.ipv4.ip_local_port_range = 32768 61000


#### Network Security Settings
# Turn on Source Address Verification in all interfaces to prevent some spoofing attacks
net.ipv4.conf.default.rp_filter=0
net.ipv4.conf.all.rp_filter=0

# Protect a server against SYN flood attacks
#Enable TCP/IP SYN cookies to
net.ipv4.tcp_syncookies=1
#Increase the maximum queue length of completely established sockets waiting to be accepted
# The net.core.somaxconn parameter is the maximum queue length of completely established sockets waiting to be accepted.
net.core.somaxconn=10000
#Increase the maximum queue length of incomplete sockets i.e. half-open connection
# The net.ipv4.tcp_max_syn_backlog parameter is the maximum queue length of incomplete sockets.
# NOTE: THis value should not be above "net.core.somaxconn", since that is also a hard open limit of maximum queue length of incomplete sockets/
# Kernel will take the lower one out of two as the maximum queue length of incomplete sockets
net.ipv4.tcp_max_syn_backlog=10000
#Increase the maximal number of TCP sockets not attached to any user file handle (i.e. orphaned connections), held by system.
# NOTE: each orphan eats up to ~64K of unswappable memory
# The net.ipv4.tcp_max_orphans parameter is the maximum number of TCP sockets not attached to any user file handle.
net.ipv4.tcp_max_orphans=10000
#Quickly Discard locally closed TCP connection
net.ipv4.tcp_orphan_retries = 2

# Protect a server against ack loop" DoS attacks
net.ipv4.tcp_invalid_ratelimit=500

# Disable packet forwarding
# # net.ipv4.ip_forward=0
# # net.ipv6.conf.all.forwarding=0


# Do not accept ICMP redirects (prevent MITM attacks)
net.ipv4.conf.all.accept_redirects = 0
net.ipv6.conf.all.accept_redirects = 0

# Do not send ICMP redirects (we are not a router)
net.ipv4.conf.all.send_redirects = 0

# Do not accept IP source route packets (we are not a router)
net.ipv4.conf.all.accept_source_route = 0
net.ipv6.conf.all.accept_source_route = 0

# Log Martian Packets
net.ipv4.conf.all.log_martians = 1

# Protect Against TCP TIME-WAIT Assassination
net.ipv4.tcp_rfc1337 = 0


### SoftIRQ Misses
# SoftIRQs are used for tasks that are not time-critical, such as network packet processing.
# Check the number of softIRQ misses
# cat /proc/net/softnet_stat
# Pay close attention to the following columns:
# Column-01: packet_process: Packet processed by each CPU.
# Column-02: packet_drop: Packets dropped.
# Column-03: time_squeeze: net_rx_action.

# Column-02: packet_drop: Packets dropped.
# Packet_drop indicates that the NIC is dropping packets due to a lack of backlog space.
# Solution 1. : Increase the size of the NIC’s backlog
# The backlog is the number of packets that the NIC can store in its backlog queue.
# Increase the backlog size to 10000
net.core.netdev_max_backlog=10000

# Column-03: time_squeeze: net_rx_action
# Time_squeeze: net_rx_action counter indicates the number of times the CPU has to return prematurely without draining the queue.
# This is most likely weak causing by weak CPU in a high-traffic environment.
# Solution 2. : Increasing net.core.netdev_budget and net.core.netdev_budget_usecs
# The netdev_budget parameter is the maximum number of packets that the kernel will process in a single softIRQ.
# The netdev_budget_usecs parameter is the maximum amount of time that the kernel will spend processing packets in a single softIRQ.
# To increase the netdev_budget and netdev_budget_usecs values, you can use the sysctl command.
net.core.netdev_budget=50000
net.core.netdev_budget_usecs=8000
# NOTE: Setting a high number might cause CPU to stall and end in poor overall performance
# NOTE: Increasing the number of Query Channels (RSS) can also help with the issue.

# Low latency busy poll timeout for socket reads
# NOTE: Not supported by most NIC
#net.core.busy_read=50
# Low latency busy poll timeout for poll and select
# NOTE: Not supported by most NIC
#net.core.busy_poll=50


### Socket buffer size
#Congestion window
# The congestion window is the amount of data that the sender can send before it must wait for an acknowledgment from the receiver.
# The congestion window is limited by 2 things.
#   The receiver’s advertised window size, which is the amount of data that the receiver is willing to accept
#   And also the size of the sending socket buffer on the sender’s end.

#How to determine the optimal congestion window
# The optimal congestion window size is determined by the bandwidth-delay product (BDP) of the network.
# The BDP is the amount of data that can be in transit in the network at any given time.
# It is calculated by multiplying the bandwidth of the network by the round-trip time (RTT) of the network.
# The optimal congestion window size is the BDP of the network.
# You can use this site to calculate the BDP of your network: https://www.speedguide.net/bdp.php

#How to determine the Optimal Receive socket Buffer Size
# The optimal socket buffer size is determined by optimal congestion window and, in turn, also determined by the bandwidth-delay product (BDP) of the network.
# We have to make sure the advertised window size is not smaller than BDP to prevent underutilization of the network.
# The receive socket buffer space is shared between the application and kernel. /
#   TCP maintains part of the buffer as the TCP window, this is the size of the receive window advertised to the other end.  /
#   The rest of the space is used as the "application" buffer, used to isolate the network from scheduling and application latencies.
# The total receive socket buffer space is determined by net.ipv4.tcp_rmem and the portion of which is allocated as "application" buffer is determined by net.ipv4.tcp_adv_win_scale.
net.ipv4.tcp_adv_win_scale=1
net.core.rmem_default=262144
net.core.rmem_max=33554432
net.ipv4.tcp_rmem=8192 262144 33554432

#How to determine the Optimal Send socket Buffer Size
# Send socket buffer size determine the maximum amount of data that the application can send before needing to wait for an acknowledgment (ACK) from the receiver
# As you may have recalled, it is bascially the definition of congestion window
# Therefore it is important to make sure the send buffer space is not smaller than BDP to prevent underutilization of the network.

# You can set send socket buffer size using the sysctl command.
net.core.wmem_default=262144
net.core.wmem_max=33554432
net.ipv4.tcp_wmem=8192 262144 33554432

#Relationship between net.core.r/wmem and net.ipv4.tcp_r/wmem
# net.core.r/wmem is the default buffer size for all protocols, including TCP
# And net.ipv4.tcp_r/wmem is the buffer size for TCP only

#net.ipv4.tcp_rmem = tcp_rmem_min tcp_rmem_default tcp_rmem_max
# Vector of 3 INTEGERs: min, default, max
#       min: Minimal size of receive buffer used by TCP sockets.
#       It is guaranteed to each TCP socket, even under moderate memory
#       pressure.
#
#       default: initial size of receive buffer used by TCP sockets.
#       This value overrides net.core.rmem_default used by other protocols.
#
#       max: maximal size of receive buffer allowed for automatically
#       selected receiver buffers for TCP socket. This value does not override
#       net.core.rmem_max.  Calling setsockopt() with SO_RCVBUF disables
#       " automatic tuning of that socket's receive buffer size, in which "
#       case this value is ignored.

#net.ipv4.tcp_wmem = tcp_wmem_min tcp_wmem_default tcp_wmem_max
# Vector of 3 INTEGERs: min, default, max
#       min: Amount of memory reserved for send buffers for TCP sockets.
#       Each TCP socket has rights to use it due to fact of its birth.
#
#       default: initial size of send buffer used by TCP sockets.  This
#       value overrides net.core.wmem_default used by other protocols.
#       It is usually lower than net.core.wmem_default.
#
#       max: Maximal amount of memory allowed for automatically tuned
#       send buffers for TCP sockets. This value does not override
#       net.core.wmem_max.  Calling setsockopt() with SO_SNDBUF disables
#       automatic tuning of that socket's send buffer size, in which case
#       this value is ignored.

# Because of the varying internet condition, not every connection is going to reach the optimal congestion window size, and that’s okay.
# To prevent slow link from using more than necessary amount of memory, we can use the following sysctl settings to enable receive buffer auto-tuning
net.ipv4.tcp_moderate_rcvbuf = 1


# Allows the use of a large window (> 64 kB) on a TCP connection, this is the default settings for most modern kernel
net.ipv4.tcp_window_scaling = 1

# Set maximum window size to MAX_TCP_WINDOW i.e. 32767 in times there is no received window scaling option
net.ipv4.tcp_workaround_signed_windows = 1


### MTU Discovery
# Allow Path MTU Discovery
net.ipv4.ip_no_pmtu_disc = 0

# Enable TCP Packetization-Layer Path, and use initial MSS of tcp_base_mss
net.ipv4.tcp_mtu_probing = 2

# Starting MSS used in Path MTU discovery
net.ipv4.tcp_base_mss = 1460

#  Minimum MSS used in connection, cap it to this value even if advertised ADVMSS option is even lower
net.ipv4.tcp_min_snd_mss = 536

# Maximum memory used to reassemble IP fragments
net.ipv4.ipfrag_high_thresh = 8388608


### Account for a high RTT lossy network
# Enable selective acknowledgments
net.ipv4.tcp_sack = 1

# Allows TCP to send "duplicate" SACKs
net.ipv4.tcp_dsack = 1

# Enable Early Retransmit. ER lowers the threshold for triggering fast retransmit when the amount of outstanding data is small and when no previously unsent data can be transmitted
net.ipv4.tcp_early_retrans = 3

# Disable ECN to survive in a congested network
net.ipv4.tcp_ecn = 0

# Reordering level of packets in a TCP stream
# Initial reordering level of packets in a TCP stream. TCP stack can then dynamically adjust flow reordering level between this initial value and tcp_max_reordering
net.ipv4.tcp_reordering = 10
# Maximal reordering level of packets in a TCP stream
net.ipv4.tcp_max_reordering = 1000
# NOTE: An attempt to reduce the number of retransmissions due to packet reordering in a network. Which is common in a lossy network

# Enable F-RTO (Forward RTO-Recovery). Beneficial in networks where the RTT fluctuates
net.ipv4.tcp_frto = 2

# Enable TCP Auto Corking
# When enabled, the TCP stack will automatically cork the socket when the application is not sending data fast enough
net.ipv4.tcp_autocorking = 1

# TCP Retry
# The number of times to retry before killing an alive TCP connection
net.ipv4.tcp_retries1 = 5
net.ipv4.tcp_retries2 = 20

# TCP Keepalive
# After  seconds of inactivity, TCP will send a keepalive probe every  to the other end. /
# After  failed attempts, the connection will be closed
# In seconds, time default value for connections to keep alive
net.ipv4.tcp_keepalive_time = 7200
# In seconds, how frequently the probes are send out
net.ipv4.tcp_keepalive_intvl = 120
# How many keepalive probes TCP sends out, until it decides that the connection is broken
net.ipv4.tcp_keepalive_probes = 15

# SYN
# Number of times SYNACKs for a passive TCP connection attempt will be retransmitted
net.ipv4.tcp_synack_retries = 10
# Number of times initial SYNs for an active TCP connection attempt     will be retransmitted
net.ipv4.tcp_syn_retries = 7


### To support more connections
#Solution 1. : Increase the maximum number of file descriptors
# The maximum number of connections that a server can handle is determined by the maximum number of file descriptors that the server can open.
#fs.file-max=655360
#fs.nr_open=655360

#Solution 2. : Increase the number of port that the kernel can allocate for outgoing connections
# The net.ipv4.ip_local_port_range parameter is the range of port numbers that the kernel can allocate for outgoing connections.
net.ipv4.ip_local_port_range=1024 65535

#Solution 3. : Increase the maximum number of sockets in TIME_WAIT state
# The net.ipv4.tcp_max_tw_buckets parameter is the maximum number of sockets in TIME_WAIT state.
net.ipv4.tcp_max_tw_buckets=10000

#Solution 4. : Quickly discard sockets in the state FIN-WAIT-2
# The net.ipv4.tcp_fin_timeout parameter is the maximum time that a connection in the FIN-WAIT-2 state will stay open.
net.ipv4.tcp_fin_timeout=10


### Miscellaneous
# Enable TCP Fast Open
# TCP Fast Open (TFO) is an extension to speed up the opening of successive TCP connections between two endpoints
net.ipv4.tcp_fastopen = 3
net.ipv4.tcp_fastopen_blackhole_timeout_sec = 0

# The maximum amount of unsent bytes in TCP socket write queue, this is on top of the congestion window
net.ipv4.tcp_notsent_lowat = 131072

# Avoid falling back to slow start after a connection goes idle
net.ipv4.tcp_slow_start_after_idle = 0

# Disable timestamps
net.ipv4.tcp_timestamps = 0

# Save cache metrics on closing connections
net.ipv4.tcp_no_metrics_save = 0

## ARP table settings
# The maximum number of bytes which may be used by packets queued for each unresolved address by other network layers
net.ipv4.neigh.default.unres_qlen_bytes = 16777216

# Controls a per TCP socket cache of one socket buffer
# net.ipv4.tcp_rx_skb_cache=1


### Buffer and cache management
# Percentage of total system memory that can be filled with dirty pages /
# before the system starts writing them to disk in the background
vm.dirty_background_ratio = 5
# Percentage of total system memory that can be filled with dirty pages
# before the system blocks any further writes /
# and forces the process that is generating dirty pages to write them to disk.
vm.dirty_ratio = 30

# The interval of when writes of dirty in-memory data are written out to disk.
# It is expressed in centiseconds
vm.dirty_writeback_centisecs = 100
# when dirty in-memory data is old enough to be eligible for writeout by the kernel flusher threads.
# It is also expressed in centiseconds.
vm.dirty_expire_centisecs = 1000

# Avoid using swap as much as possible
vm.swappiness = 10


### Congestion Control
net.core.default_qdisc = fq
net.ipv4.tcp_congestion_control = bbr


# >
# # >
# Avoid falling back to slow start after a connection goes idle
net.ipv4.tcp_slow_start_after_idle=0
# Because of the varying internet condition, not every connection is going to reach the optimal congestion window size, and that’s okay.
# To prevent slow link from using more than necessary amount of memory, we can use the following sysctl settings to enable receive buffer auto-tuning
net.ipv4.tcp_moderate_rcvbuf=1
net.ipv4.tcp_adv_win_scale=1
net.core.default_qdisc=fq_pie
net.core.rmem_default=262144
net.core.wmem_default=262144
net.core.rmem_max=67108848
net.core.wmem_max=67108848
net.ipv4.tcp_rmem=8192 262144 67108848
net.ipv4.tcp_wmem=8192 262144 67108848
# # # net.ipv4.route.flush=1


# 系统会等待一定时间来聚合小包，这可能会 "增加数据传输的延迟"
# 但是可以 "提高网络传输效率"
# 对于实时性要求高的应用: (如在线游戏 , VoIP和视频会议 , 股票交易系统 , 物联网设备控制-智能家居)
# 0 是关闭 ; 1 是开启
net.ipv4.tcp_autocorking = 0


# # Swap 优化
# vm.swappiness=10
# # 保留更多的 VFS 缓存，从而提高文件系统操作的效率
# vm.vfs_cache_pressure=50
#
# # rfs 设置此文件至同时活跃连接数的最大预期值
# net.core.rps_sock_flow_entries=32768


# # 禁止 ping
# # 系统将忽略所有收到的 ICMP 回显请求 (ping) 消息
# net.ipv4.icmp_echo_ignore_all=1
#
# # 系统将忽略广播或多播地址发送的 ping 请求
# # 这是许多 Linux 发行版的默认设置
# net.ipv4.icmp_echo_ignore_broadcasts=1


# # > BPECS 
# vm.swappiness = 0
# kernel.sysrq = 1
# net.ipv4.neigh.default.gc_stale_time = 120
# net.ipv4.conf.all.rp_filter = 0
# net.ipv4.conf.default.rp_filter = 0
# net.ipv4.conf.default.arp_announce = 2
# net.ipv4.conf.lo.arp_announce = 2
# net.ipv4.conf.all.arp_announce = 2
# net.ipv4.tcp_max_tw_buckets = 5000
# net.ipv4.tcp_syncookies = 1
# net.ipv4.tcp_max_syn_backlog = 1024
# net.ipv4.tcp_synack_retries = 2


